\section{Wnioski}

% Złożność algorytmów
\subsection{Złożoność obliczeniowa zaproponowanego algorytmu heurystycznego}

Zaproponowany przez autorów algorytm charakteryzuje się bardzo niewielkim
czasem wykonania obliczeń. Złożoność tą można bardzo łatwo wyznaczyć.
Działanie algorytmu opierasię, jak wspomniano we wstępie na wyborze
odpowiedniego punktu spośród dostępnych. Przy początkowym punkcie, gdy
dostępnych jest $n - 1$ punktów (ponieważ jeden już jest wykorzystany - 
został wylosowany). Spośród tych punktów należy wybrać minimum i
z nowego punktu ponownie wybrać najbliższy punkt na podstawie wag
poszczególnych łuków w grafie. Należy wykonać kolejno $n - 1$
takich operacji ze względu, na fakt że dla ostatniego 
wierzchołka takiej operacji wykonywać już nie trzeba. Prowadzi to do
złożoności kwadratowej, którą można opisać jako $ O(n^{2}) $.
Dla analizowanych małych instancji problemu komiwojażera, złożoność ta
skutkuje błyskawicznym czasem wykonywania obliczeń.

Warto zwrócić uwagę na fakt, iż można dokonać niewielkiej optymalizacji,
która wprawdzie nie zmniejszy rzędu wielkości złożoności problemu, jednak
może skutkować nieznacznym zmniejszeniem złożoności. W przypadku gdy został
wybrany wierzchołek pewne jest, że nie może zostać on odwiedzony ponownie
dlatego możliwe jest usunięcie go z macierzy odległości między
wierzchołkami, a więc usunięcie kolumny oraz wiersza powiązanych z
analizowanym wierzchołkiem. Jeżeli pominąć czas wykonania
operacji usuwania wierzy i kolumn, skutkowałoby to wykonaniem dla pierwszego
wierzchołka $n - 1$ operacji przy wyznaczaniu minimum, ale dla kolejnego
wierzchołka byłyoby tych elementów $n - 2$, a dla następnego $n - 3$.
Można zauważyć, że złożoność wyniosłaby wówczas $\frac{n \cdot (n - 1)}{2}$.
Złożoność jest więc określona, a więc wiadomo, że w jakim czasie
można się spodziewać rozwiązania takiego problemu.

Wadą tego rozwiąznaia jest
możliwość otrzymania wyniku bardzo oddalonego od optimum. Dla specyficznych
danych możliwe jest, 
że przy końcowych obliczeniach, gdy nie mam już dużego wyboru następnych
odwiedzanych punktów, więc ostatnie ścieżki do wyboru mogą być bardzo długie.
Wynik działania tego algorytmu mógłby być podstawą do uruchomienia
algorytmów Steepest lub Greedy, które sprawdziłyby, czy w sąsiedztwie
nie leżą lepsze rozwiązania. Znacznie skróciło
by to czas działania, poprzez start w miejscu, które z dużym
prawdopodobieństwem znajduje się blisko optimum lub lokalnym minimum.
Istnieje możliwość, że punkt początkowy leżałby w pobliżu optimum
lokalnego, i zawsze zwracane byłoby to samo rozwiązanie, co
uniemożliwiając odkrycie
innych minimów lokalnych, a co za tym idzie
poprawę rezultatu końcowego.

\subsection{Odległość od optimum}

Zobrazowane na wykresach wyniki pozwalają dostrzec, że odległości od
optimum rozwiązań zwróconych przez badane algorytmy
są bardzo zróżnicowane. Najbardziej stalinie prezentuje się algorytm autorski. Algorytmy Steepest i Greedy lepiej działają dla pewnych instancji
o dużych rozmiarach.

Najbardziej zmienny jest algorytm losowy. Dane uzyskane za jego pomocą 
la małych instancji mogą być bliskie optimum, a dla dużych instancji
są bardzo odległe. Rozwiązanie tego specyficznego zachowania zostało
wytłumaczone poniżej.

\subsection{Uzyskane wyniki}

Pomiary jakości oraz szybkości wykonywania poszczególnych algorytmów
pozwalają stwierdzić, że najlepiej zarówno pod względem czasowym, jak i
jakościowym najlepiej poradził się zaproponowany
przez autorów algorytm heurystyczny. Pozostałe algorytmy poradziły sobie
nieco gorzej. Najsłabsze rezultaty uzyskano w przypadku algorytmu
opartego o losowanie rozwiązań z całej przestrzeni rozwiązań dopuszczalnych.

\subsubsection{Zaproponowany algorytm heurystyczny}

Efekty działania algorytmu autorskiego okazują się być bardzo
interesujące. Przyczyną dobrych wyników może być podejście do problemu,
które poniekąd uwzględnia specyfikę problemu komiwojażera.
Zastosowanie algorytmu zachłannego w przypadku tworzenia rozwiązania,
pozwoliło osiągnąć bardzo dobre rezultaty, zarówno czasowe, jak i
jakościowe. Zaproponowana implementacja opiera się na dodawaniu
najbliższego punktu od obecnie odwiedzanego. Instancje problemu komiwojażera,
które są wygenerowane na podstawie obliczania rzeczywistej odległości
pomiędzy punktami na płaszczyźnie bardzo dobrze pasują do zastosowania
przy ich rozwiązaniu algorytmu opierającego się o zachłanne dodawanie
punktów ze zbioru jeszcze niewykorzystanych.
Możliwe są jednak przypadki, kiedy zaproponowany przez autorów
algorytm heurystyczny nie sprawdzi się przy tworzeniu rozwiązań
problemu komiwojażera. Dla instancji, w których ostatnia wybrana
ścieżka jest bardzo długa. Po otrzymaniu rozwiązania można by było
przejrzeć sąsiedztwo i z tego wybrać najlepsze rozwiązania. Można byłoby
również zastosować rozwiązanie mieszane, tzn. takie w którym do
wyznaczenia rozwiązania początkowego posłużyłby algorytm autorski,
ale dalsze obliczenia mogłyby być wykonane zgodnie z jednym
spośród tych umożliwiających poszukiwanie rozwiązań w sąsiedztwie
(np. algorytm  $Steepest$ lub algorytm $Greedy$).

\subsubsection{Algorytm Greedy i Steepest}

Oba algorytmy pozwalają przeglądać sąsiedztwo rozwiązania początkowego
oraz podążać w poszukiwaniu jednego z minimów lokalnych.
Można dostrzec, że algorytm $Greedy$ w większości przypadków dostarcza
rozwiązania lepsze od algorytmu $Steepest$. Dzieje się tak prawdopodobnie
ze względu na fakt, iż algorytm $Steepest$ przeszukuje całe sąsiedztwo
analizowanego rozwiązania i wybiera z niego najlepsze możliwe. Skutkuje
to znajdowaniem najbliższego minimum lokalnego. W przypadku algorytmu
$Greedy$ dochodzi swojego rodzaju losowość w wyborze rozwiązania
sąsiedniego poddawanego analizie. Nigdy bowiem nie wiadomo, jakie
rozwiązanie będzie pierwszym analizowanym przez algorytm $Greedy$,
a więc nie musi to być rozwiązanie o najniższej wartości długości
ścieżki spośród całego sąsiedztwa. Pozwala to, by nie zbliżać
się zbyt szybko w kierunku lokalnego minimum.

\subsubsection{Algorytm losowy}

Jak wspomniano wcześniej, algorytm losowy zwracał najgorsze rozwiązania spośród
opisywanych algorytmów. Słabość tego algorytmu wiąże się z jego niedeterminizmem
oraz brak możliwości wykorzystania sprawdzonych rozwiązań do poprawy jakości
rozwiązań. Algorytm ten nie oferuje przeszukiwania lokalnego. Każde rozwiązanie
generowane jest od nowa, co skutkuje brakiem systematycznego przybliżania
się do optimum, zarówno lokalnego, jak i rozwiązania optymalnego.

W przypadku małych instancji danych wejściowych algorytm zwraca wyniki, które
dużo bardziej pasują do oczekiwanych rezultatów. Niestety wzrost rozmiaru
instancji powoduje, że otrzymywane rozwiązania stają się co raz gorsze
pomimo zwiększania ilości losowań. Taki stan rzeczy jest ściśle
związany z bardzo dużą przestrzenią rozwiązań, która rośnie wykładniczo
w zależności od rozmiaru instancji.

Prawdopodobieństwo znalezienia rozwiązania optymalnego przez algorytm
losowy w przypadku dużych instancji problemu wydaje się
być niemal zerowe. Liczba permutacji ustawień poszczególnych wierzchołków
z grafu danych wejściowych jest bardzo duża, a osiągnięcie odpowiednich
rezultatów wiązałoby się z ogromną ilością losowań, co byłoby niezmiernie
mało efektywne.

\subsubsection {Algorytm Tabu Search}

Algorytm $Tabu Search$ w ogólności zwracał wyniki bliższe optimum od
prezentowanych wcześniej algorytmów przeszukiwania lokalnego. Może to być
zwizane z mechanizmem, który ma zapobiegać przed wpadaniem w minimum
lokalne. Podczas analizy algorytmu wykonano także eksperymenty, w których
w różny w różny sposób traktowano rozwiązanie początkowe. W rozwiązaniu
klasycznym, jako punkt startowy wybierane było rozwiązanie losowe,
natomiast drugie podejście opierało się na wykorzystaniu algorytmu,
przedstawionego w poprzedniej części nazwanego jako utorska heurystyka.
Jak można zaobserwować na rysunku \ref{tabu_quality_2} różnica w
zwracanych wynikach jest znacząca, na korzyść rozwiązania z wprowadzoną
optymalizacją (na wykresie opisane jako ``tabu search 2''.
Taki stan rzeczy związany jest z wyborem rozwiąznaia początkowego,
które z dużym prawdopodobieństwem znajduje się blisko rozwiązania
optymalnego i być może jest jednym z minimów lokalnych. Zasotosowanie
algorytmu opartego o listę tabu pozwala jednak ``wyskoczyć'' z lolalnego
minimum i skupić się na innych fragmentach obszaru poszukiwania
rozwiązania optymalnego.

Oprócz analizy różnego punktu startowego, dla algorytmu $Tabu Search$
przeprowadzono także eksperyment w którym porównano jakość rozwiązań
w zależności od rozmiaru listy tabu. Niestety w czasie przeprowadzania
eksperymentu popełniono pomyłkę i uruchomiony został algorytm w wersji,
w której za rozwiązanie początkowe wybierane było rozwiązanie zwracane
przez własną heurystykę opisaną we wcześniejszej części. Wyniki
zaprezentowane w tabeli nr \ref{diferent_tabu_size_table} oraz
na wykresach nr \ref{tabu_quality_size_1}, \ref{tabu_quality_size_2},
\ref{tabu_quality_size_3}, \ref{tabu_quality_size_4} pozwalają
zauważyć, że rozmiar listy tabu ma wpływ na jakoś uzyskiwanych 
rezultatów. Być może, gdyby zastosować wersję, w której rozwiązaniem
początkowym jest rozwiązanie losowe. Jak jednak wspomniano, wiadać że
dla pewnych rozmiarów instancji uzyskiwane rezultaty są dokładniejsze.
Analizując wyniki z tabeli nr \ref{diferent_tabu_size_table}
zaobserować można, że najlepsze wyniki uzyskano w momencie gdy rozmiar
listy tabu osiągnąła wartość 80. Nie da się również ukryć że stosowanie
dużych list tabu odbija się negatywnie na czasie wykonywania się
algorytmu. Zwiększenie rozmiaru listy wiąże się ze wzrostem ilości
operacji związanych z jej przeglądaniem. Złożoność
przeszukiwania listy jest liniowa (O(n)). Warto zauważyć że jest to
jednak ściśle związane ze sposobem implementacji przeszukiwania
na liście tabu i interpretacji dosłownej. Lista tabu może być przecież
reprezentowana w innej strukturze niż struktura listowa. Wystarczy,
że lista będzie reprezentowana w postaci tablicy hasho'owej oraz
kolejki FIFO. Do tablicy hash'owej trafiałyby kolejne rekordy
reprezentujące zakazane ruchy, a kolejka FIFO służyłaby do określania,
które elementy z tablicy hasho'wej należy usunąć. Złożoność takiego
rozwiązania byłaby wówczas stała O(1). Bez względu na rozmiar długość
tabu sprawdzanie dostęp do każdego elementu w przypadku średnim
jest stały. Należy jendka pamiętać o nieco większym narzucie pamięciowym,
gdyż oprócz tablicy hash'owej przechowywana musib być także kolejka FIFO.


\subsubsection {Algorytm Simulated Anealing}

Algorytm $Tabu Search$ w ogólności zwracał wyniki bliższe optimum od
prezentowanych wcześniej algorytmów przeszukiwania lokalnego. Symulowane
wyrzażanie pozwala na wyjście z minimum lokalnego w przypadku
kiedy algorytm na takie rozwiązanie się natknie. Prezentowane wcześniej
algorytmy przeszukiwania lokalnego kończą działanie w przypadku
odnalezienia minimum lokalnego, z czym związany jest także krótszy
czas działania.

Dla przedstawianego algorytmu wykonano także analizę działania
w zależności od wartości temperatury
początkowej. W przypadku małej wartości temperatury początkowej,
działanie algorytmu nie satysfakcjonuje. Można zaobserwować słabe
rezultaty, a więc dużą odległość od rozwiązania optymalnego. Wraz
ze wzrostem temperatury początkowej rośnie czas przetwarzania, ilość
odwiedzonych rozwiązań dopuszczalnych oraz rezultaty zwracane przez
algorytm stają się bliższe optymalnym wynikom.
Czas działania algorytmu jest zbliżony dla wszystkich instancji problemu.
Nie trudno zauważyć, że skoro głównym warunkiem stopu algorytmu jest
spadek temperatury do progowej wartości. Jeżeli założyć, że temperatura
początkowa jest taka sama dla każdej instancji problemu, bez względu
na jej rozmiar, czas osiągnięcia warunku stopu w postaci spadku
temperatury jest więc zbliżony.

\subsubsection {Simulated Anealing i Tabu Search}

Na rysunku nr \ref{anealing_tabu_quality} przedstawiono porównanie
wyników algorytmów $Simulated Anealing$ oraz $Tabu Search$ dla wersji
bez optymalizacji doboru rozwiązania początkowego, a więc w
formie, w której rozwiązanie początkowe jest losowane. Trudno
jednoznacznie określić, które z zaprezentowanych rozwiązań zwraca
lepsze rezultaty rozwiązań. Nie ulega jednak wątpliwości, że
algorytm $Tabu Search$ zakończył działanie w czasie krótszym od
algorytmu $Simulated Anealing$. Można to zaobserwować na rysunku nr
\ref{anealing_tabu_greedy_time_log}. Warto również zauważyć, że oba
algorytmy wykazują dłuższy czas przetwarzania niż badane uprzednio
algorytmy przeszukiwania lokalnego. Należy jednak zwrócić uwagę,
że uzyskane rezultaty są ściśle zależne od parametrów charakteryzujących
poszczególne algorytmy. Trudno jest w takiej sytuacji porównywać
rozwiązania, które nie mają wspólnych parametrów, odpowiadających np.
za warunek stopu.

W przypadku prezentowanych algorytmów można zauważyć jeszcze
jedną prawidłowość
odnośnie ilości popraw rozwiązań w czasie przebiegu działania
algorytmu. Porównując te liczby z przedstawionymi wcześniej
algorytmami przeszukiwania lokalnego, można zauważyć, że algorytm
$Tabu Search$ dokonuje odnalezienia podobnej ilości popraw
bieżącego najlepszego rozwiązania jak w przypadku algorytmu $Steepest$.
Algorytm $Simulated Anealing$ pozwala na odnalezienie dużo mniejszej
liczby rozwiązań poprawiających obecnie najlepszą wartość. Wygląda
jednak na to, że nie przeszkadza to w znajdywaniu rozwiązań
dokładniejszych niż wspomniany algorytm $Steepest$.

\subsection{Czas wykonywania algorytmów}

W zestawieniu nie został ujęty prosty algorytm heurystyczny z
powodu zbyt krótkich czasów wykonywania. Jak wspomniano wcześniej
algorytm nie opiera się na przeszukiwaniu sąsiedztwa, dlatego
ilość wykonywanych przez niego operacji jest ograniczona do
minimum. W czasie działania algorytmu tylko raz wykonywana
jest procedura obliczania długości utworzonej ścieżki -- dopiero
przy końcu działania. W algorytmach przeszukujących sąsiedztwo
wykonywanie tej procedury jest podstawowym elementem przetwarzania,
gdyż pozwala stwierdzić w jakim kierunku podążać.


\subsubsection{Algorytm Greedy}

Czas działania algorytmu dla dużych instancji problemu jest lepszy
w porównaniu do algorytmu Steepest. Pomimo, że potrzebuje on
więcej iteracji, przejście do następnego kroku
zajmuje mniej czasu, gdyż następuje od razu po znalezieniu
lepszego od dotychczasowego rozwiązania.

\subsubsection{Algorytm Steepest}

Algorytm zwraca w większości przypadków rezultaty gorsze
od algorytmu Greedy. Wynika to głównie z wspomnianego
wcześniej faktu, że przegląda on
wszystkie rozwiązania zanim zostanie podjęta decyzja o następnym kroku.

\subsubsection{Algorytm losowy}

Algorytm ten działa niederministycznie, dlatego bardzo trudno
przewidzieć zwracane wyniki. Czas działania tego algorytmu
jest z kolei łatwy do przewidzenia, gdyż jest ściśle uzależniony
od ilości losowań, które muszą zostać wykonane. Dodatkowo na czas
działania algorytmu wpływa także rozmiar instancji. Od rozmiaru
instancji zależy szybkość wykonywania funkcji oceniającej.

\subsubsection {Algorytm Simulated Annealing}

Algorytm Simulated Annealing posiada trzy warunki zakończenia: osiągnięcie 
założonej ilości kroków, osiągnięcie założonej liczby kroków bez poprawy 
rozwiązania oraz opadnięcie temperatury do określonej wartości. Ograniczenia na
ilość kroków autorzy dobrali mniej wiecej tak, żeby to temperatura determinowała
zakończenie algorytmu. Zaprezentowane jest to na wykresie \ref{annealing1}, na
którym to widać, jak temperatura wpływała na czas działania algorytmu. 
Przetestowano ten algorytm dla tej samej instancji wejściowej przyjmując różne
wartości temperatury. Widać więc, że wraz ze wzrostem temperatury rośnie czas
działania tego algorytmu.
Na wykresie \ref{anealing_tabu_greedy_time_log} widać natomiast, że w przypadku, 
gdy temperatura jest stała, a zmienia się instancja wejściowa czas działania 
algorytmu zmienia się nieznacznie, co jest uzależnione od wielkości instancji
wejściowej i jej skomplikowania.

\subsubsection {Algorytm Tabu Search}

Za warunek stopu algorytmu Tabu Search uważa się określoną ilość kroków, jakie
musi przebiec algorytm. Jak wiadomo im więcej kroków algorytm musi wykonać tym
czas działania algorytmu jest większy. 
Na działanie tego algorytmu ma także wpływ wielkość tablicy tabu,
którą algorytm musi przejrzeć w każdym kroku aby stwierdzić, czy może przejść
do wyznaczonego rozwiązania sąsiedniego. Pokazuje to tabela 
\ref{diferent_tabu_size_table}. Im większa tablica tabu tym dłużej działa 
algorytm.

\subsection{Ilość iteracji}

Porównując ilość iteracji algorytmów Greedy i Steepest można zauważyć,
że algorytm $Steepest$ wykonuje mniej kroków od algorytmu $Greedy$.
Jest to spowodowane dokładnym przeglądaniem sąsiedztwa i wybierania
najlepszego rozwiązania.
