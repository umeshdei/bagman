\section{Wnioski}

% Złożność algorytmów
\subsection{Złożoność obliczeniowa zaproponowanego algorytmu heurystycznego}

Zaproponowany przez autorów algorytm charakteryzuje się bardzo niewielkim
czasem wykonania obliczeń. Złożoność tą można bardzo łatwo wyznaczyć.
Działanie algorytmu opierasię, jak wspomniano we wstępie na wyborze
odpowiedniego punktu spośród dostępnych. Przy początkowym punkcie, gdy
dostępnych jest $n - 1$ punktów (ponieważ jeden już jest wykorzystany - 
został wylosowany). Spośród tych punktów należy wybrać minimum i
z nowego punktu ponownie wybrać najbliższy punkt na podstawie wag
poszczególnych łuków w grafie. Należy wykonać kolejno $n - 1$
takich operacji ze względu, na fakt że dla ostatniego 
wierzchołka takiej operacji wykonywać już nie trzeba. Prowadzi to do
złożoności kwadratowej, którą można opisać jako $ O(n^{2}) $.
Dla analizowanych małych instancji problemu komiwojażera, złożoność ta
skutkuje błyskawicznym czasem wykonywania obliczeń.

Warto zwrócić uwagę na fakt, iż można dokonać niewielkiej optymalizacji,
która wprawdzie nie zmniejszy rzędu wielkości złożoności problemu, jednak
może skutkować nieznacznym zmniejszeniem złożoności. W przypadku gdy został
wybrany wierzchołek pewne jest, że nie może zostać on odwiedzony ponownie
dlatego możliwe jest usunięcie go z macierzy odległości między
wierzchołkami, a więc usunięcie kolumny oraz wiersza powiązanych z
analizowanym wierzchołkiem. Jeżeli pominąć czas wykonania
operacji usuwania wierzy i kolumn, skutkowałoby to wykonaniem dla pierwszego
wierzchołka $n - 1$ operacji przy wyznaczaniu minimum, ale dla kolejnego
wierzchołka byłyoby tych elementów $n - 2$, a dla następnego $n - 3$.
Można zauważyć, że złożoność wyniosłaby wówczas $\frac{n \cdot (n - 1)}{2}$.
Złożoność jest więc określona, a więc wiadomo, że w jakim czasie
można się spodziewać rozwiązania takiego problemu.

Wadą tego rozwiąznaia jest
możliwość otrzymania wyniku bardzo oddalonego od optimum. Dla specyficznych
danych możliwe jest, 
że przy końcowych obliczeniach, gdy nie mam już dużego wyboru następnych
odwiedzanych punktów, więc ostatnie ścieżki do wyboru mogą być bardzo długie.
Wynik działania tego algorytmu mógłby być podstawą do uruchomienia
algorytmów Steepest lub Greedy, które sprawdziłyby, czy w sąsiedztwie
nie leżą lepsze rozwiązania. Znacznie skróciło
by to czas działania, poprzez start w miejscu, które z dużym
prawdopodobieństwem znajduje się blisko optimum lub lokalnym minimum.
Istnieje możliwość, że punkt początkowy leżałby w pobliżu optimum
lokalnego, i zawsze zwracane byłoby to samo rozwiązanie, co
uniemożliwiając odkrycie
innych minimów lokalnych, a co za tym idzie
poprawę rezultatu końcowego.

\subsection{Odległość od optimum}

Zobrazowane na wykresach wyniki pozwalają dostrzec, że odległości od
optimum rozwiązań zwróconych przez badane algorytmy
są bardzo zróżnicowane. Najbardziej stalinie prezentuje się algorytm autorski. Algorytmy Steepest i Greedy lepiej działają dla pewnych instancji
o dużych rozmiarach.

Najbardziej zmienny jest algorytm losowy. Dane uzyskane za jego pomocą 
la małych instancji mogą być bliskie optimum, a dla dużych instancji
są bardzo odległe. Rozwiązanie tego specyficznego zachowania zostało
wytłumaczone poniżej.

\subsection{Uzyskane wyniki}

Pomiary jakości oraz szybkości wykonywania poszczególnych algorytmów
pozwalają stwierdzić, że najlepiej zarówno pod względem czasowym, jak i
jakościowym najlepiej poradził się zaproponowany
przez autorów algorytm heurystyczny. Pozostałe algorytmy poradziły sobie
nieco gorzej. Najsłabsze rezultaty uzyskano w przypadku algorytmu
opartego o losowanie rozwiązań z całej przestrzeni rozwiązań dopuszczalnych.

\subsubsection{Zaproponowany algorytm heurystyczny}

Efekty działania algorytmu autorskiego okazują się być bardzo
interesujące. Przyczyną dobrych wyników może być podejście do problemu,
które poniekąd uwzględnia specyfikę problemu komiwojażera.
Zastosowanie algorytmu zachłannego w przypadku tworzenia rozwiązania,
pozwoliło osiągnąć bardzo dobre rezultaty, zarówno czasowe, jak i
jakościowe. Zaproponowana implementacja opiera się na dodawaniu
najbliższego punktu od obecnie odwiedzanego. Instancje problemu komiwojażera,
które są wygenerowane na podstawie obliczania rzeczywistej odległości
pomiędzy punktami na płaszczyźnie bardzo dobrze pasują do zastosowania
przy ich rozwiązaniu algorytmu opierającego się o zachłanne dodawanie
punktów ze zbioru jeszcze niewykorzystanych.
Możliwe są jednak przypadki, kiedy zaproponowany przez autorów
algorytm heurystyczny nie sprawdzi się przy tworzeniu rozwiązań
problemu komiwojażera. Dla instancji, w których ostatnia wybrana
ścieżka jest bardzo długa. Po otrzymaniu rozwiązania można by było
przejrzeć sąsiedztwo i z tego wybrać najlepsze rozwiązania. Można byłoby
również zastosować rozwiązanie mieszane, tzn. takie w którym do
wyznaczenia rozwiązania początkowego posłużyłby algorytm autorski,
ale dalsze obliczenia mogłyby być wykonane zgodnie z jednym
spośród tych umożliwiających poszukiwanie rozwiązań w sąsiedztwie
(np. algorytm  $Steepest$ lub algorytm $Greedy$).

\subsubsection{Algorytm Greedy i Steepest}

Oba algorytmy pozwalają przeglądać sąsiedztwo rozwiązania początkowego
oraz podążać w poszukiwaniu jednego z minimów lokalnych.
Można dostrzec, że algorytm $Greedy$ w większości przypadków dostarcza
rozwiązania lepsze od algorytmu $Steepest$. Dzieje się tak prawdopodobnie
ze względu na fakt, iż algorytm $Steepest$ przeszukuje całe sąsiedztwo
analizowanego rozwiązania i wybiera z niego najlepsze możliwe. Skutkuje
to znajdowaniem najbliższego minimum lokalnego. W przypadku algorytmu
$Greedy$ dochodzi swojego rodzaju losowość w wyborze rozwiązania
sąsiedniego poddawanego analizie. Nigdy bowiem nie wiadomo, jakie
rozwiązanie będzie pierwszym analizowanym przez algorytm $Greedy$,
a więc nie musi to być rozwiązanie o najniższej wartości długości
ścieżki spośród całego sąsiedztwa. Pozwala to, by nie zbliżać
się zbyt szybko w kierunku lokalnego minimum.

\subsubsection{Algorytm losowy}

Jak wspomniano wcześniej, algorytm losowy zwracał najgorsze rozwiązania spośród
opisywanych algorytmów. Słabość tego algorytmu wiąże się z jego niedeterminizmem
oraz brak możliwości wykorzystania sprawdzonych rozwiązań do poprawy jakości
rozwiązań. Algorytm ten nie oferuje przeszukiwania lokalnego. Każde rozwiązanie
generowane jest od nowa, co skutkuje brakiem systematycznego przybliżania
się do optimum, zarówno lokalnego, jak i rozwiązania optymalnego.

W przypadku małych instancji danych wejściowych algorytm zwraca wyniki, które
dużo bardziej pasują do oczekiwanych rezultatów. Niestety wzrost rozmiaru
instancji powoduje, że otrzymywane rozwiązania stają się co raz gorsze
pomimo zwiększania ilości losowań. Taki stan rzeczy jest ściśle
związany z bardzo dużą przestrzenią rozwiązań, która rośnie wykładniczo
w zależności od rozmiaru instancji.

Prawdopodobieństwo znalezienia rozwiązania optymalnego przez algorytm
losowy w przypadku dużych instancji problemu wydaje się
być niemal zerowe. Liczba permutacji ustawień poszczególnych wierzchołków
z grafu danych wejściowych jest bardzo duża, a osiągnięcie odpowiednich
rezultatów wiązałoby się z ogromną ilością losowań, co byłoby niezmiernie
mało efektywne.

\subsection {Algorytm Tabu Search}

Algorytm $Tabu Search$ w ogólności zwracał wyniki bliższe optimum od
prezentowanych wcześniej algorytmów przeszukiwania lokalnego. Może to być
zwizane z mechanizmem, który ma zapobiegać przed wpadaniem w minimum
lokalne. Podczas analizy algorytmu wykonano także eksperymenty, w których
w różny w różny sposób traktowano rozwiązanie początkowe. W rozwiązaniu
klasycznym, jako punkt startowy wybierane było rozwiązanie losowe,
natomiast drugie podejście opierało się na wykorzystaniu algorytmu,
przedstawionego w poprzedniej części nazwanego jako utorska heurystyka.
Jak można zaobserwować na rysunku \ref{tabu_quality_2} różnica w
zwracanych wynikach jest znacząca, na korzyść rozwiązania z wprowadzoną
optymalizacją (na wykresie opisane jako ``tabu search 2''.
Taki stan rzeczy związany jest z wyborem rozwiąznaia początkowego,
które z dużym prawdopodobieństwem znajduje się blisko rozwiązania
optymalnego i być może jest jednym z minimów lokalnych. Zasotosowanie
algorytmu opartego o listę tabu pozwala jednak ``wyskoczyć'' z lolalnego
minimum i skupić się na innych fragmentach obszaru poszukiwania
rozwiązania optymalnego.

Oprócz analizy różnego punktu startowego, dla algorytmu $Tabu Search$
przeprowadzono także eksperyment w którym porównano jakość rozwiązań
w zależności od rozmiaru listy tabu. Niestety w czasie przeprowadzania
eksperymentu popełniono pomyłkę i uruchomiony został algorytm w wersji,
w której za rozwiązanie początkowe wybierane było rozwiązanie zwracane
przez własną heurystykę opisaną we wcześniejszej części.


\subsection {Algorytm Simulated Anealing}

Algorytm $Tabu Search$ w ogólności zwracał wyniki bliższe optimum od
prezentowanych wcześniej algorytmów przeszukiwania lokalnego.

Wykonano także analizę działania algorytmu w zależności od temperatury
początkowej. W przypadku małej wartości temperatury początkowej,
działanie algorytmu nie satysfakcjonuje. Można zaobserwować słabe
rezultaty, a więc dużą odległość od rozwiązania optymalnego. Wraz
ze wzrostem temperatury początkowej rośnie czas przetwarzania, ilość
odwiedzonych rozwiązań dopuszczalnych oraz rezultaty zwracane przez
algorytm stają się bliższe optymalnym wynikom.

Czas działania algorytmu jest zbliżony dla wszystkich instancji problemu.
Nie trudno zauważyć, że skoro głównym warunkiem stopu algorytmu jest
spadek temperatury do progowej wartości. Jeżeli założyć, że temperatura
początkowa jest taka sama dla każdej instancji problemu, bez względu
na jej rozmiar, czas osiągnięcia warunku stopu w postaci spadku
temperatury jest więc zbliżony.

\subsection {Simulated Anealing i Tabu Search}

Na rysunku nr \ref{anealing_tabu_quality} przedstawiono porównanie
wyników algorytmów $Simulated Anealing$ oraz $Tabu Search$ dla wersji
bez optymalizacji doboru rozwiązania początkowego, a więc w
formie, w której rozwiązanie początkowe jest losowane. Trudno
jednoznacznie określić, które z zaprezentowanych rozwiązań zwraca
lepsze rezultaty rozwiązań. Nie ulega jednak wątpliwości, że
algorytm $Tabu Search$ zakończył działanie w czasie krótszym od
algorytmu $Simulated Anealing$. Można to zaobserwować na rysunku nr
\ref{anealing_tabu_greedy_time_log}. Warto również zauważyć, że oba
algorytmy wykazują dłuższy czas przetwarzania niż badane uprzednio
algorytmy przeszukiwania lokalnego. Należy jednak zwrócić uwagę,
że uzyskane rezultaty są ściśle zależne od parametrów charakteryzujących
poszczególne algorytmy. Trudno jest w takiej sytuacji porównywać
rozwiązania, które nie mają wspólnych parametrów, odpowiadających np.
za warunek stopu.


\subsection{Czas wykonywania algorytmów}

W zestawieniu nie został ujęty prosty algorytm heurystyczny z
powodu zbyt krótkich czasów wykonywania. Jak wspomniano wcześniej
algorytm nie opiera się na przeszukiwaniu sąsiedztwa, dlatego
ilość wykonywanych przez niego operacji jest ograniczona do
minimum. W czasie działania algorytmu tylko raz wykonywana
jest procedura obliczania długości utworzonej ścieżki -- dopiero
przy końcu działania. W algorytmach przeszukujących sąsiedztwo
wykonywanie tej procedury jest podstawowym elementem przetwarzania,
gdyż pozwala stwierdzić w jakim kierunku podążać.


\subsubsection{Algorytm Greedy}

Czas działania algorytmu dla dużych instancji problemu jest lepszy
w porównaniu do algorytmu Steepest. Pomimo, że potrzebuje on
więcej iteracji, przejście do następnego kroku
zajmuje mniej czasu, gdyż następuje od razu po znalezieniu
lepszego od dotychczasowego rozwiązania.

\subsubsection{Algorytm Steepest}

Algorytm zwraca w większości przypadków rezultaty gorsze
od algorytmu Greedy. Wynika to głównie z wspomnianego
wcześniej faktu, że przegląda on
wszystkie rozwiązania zanim zostanie podjęta decyzja o następnym kroku.

\subsubsection{Algorytm losowy}

Algorytm ten działa niederministycznie, dlatego bardzo trudno
przewidzieć zwracane wyniki. Czas działania tego algorytmu
jest z kolei łatwy do przewidzenia, gdyż jest ściśle uzależniony
od ilości losowań, które muszą zostać wykonane. Dodatkowo na czas
działania algorytmu wpływa także rozmiar instancji. Od rozmiaru
instancji zależy szybkość wykonywania funkcji oceniającej.

\subsection {Algorytm Tabu Search}



\subsection {Algorytm Simulated Anealing}

\subsection{Ilość iteracji}

Porównując ilość iteracji algorytmów Greedy i Steepest można zauważyć,
że algorytm $Steepest$ wykonuje mniej kroków od algorytmu $Greedy$.
Jest to spowodowane dokładnym przeglądaniem sąsiedztwa i wybierania
najlepszego rozwiązania.
